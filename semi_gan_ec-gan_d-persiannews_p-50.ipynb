{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semi_gan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamardian/digikalamag/blob/main/semi_gan_ec-gan_d-persiannews_p-50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVzghhQznhMw",
        "outputId": "57950113-98d8-47a5-bc14-378579accf1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "(130) train_accuracy : 15.865384615384615\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 16.294642857142858\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 15.416666666666666\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 15.4296875\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 15.441176470588236\n",
            "  Batch   170  of    209.    Elapsed: 0:03:50.\n",
            "(180) train_accuracy : 15.79861111111111\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 15.625\n",
            "  Batch   190  of    209.    Elapsed: 0:04:17.\n",
            "(200) train_accuracy : 15.78125\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 30 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "227 / 1644 * 100 = 13.80778588807786 \n",
            "\n",
            "======== Epoch 31 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 18.75\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 18.75\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 19.791666666666668\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 16.40625\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 13.75\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 15.104166666666666\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 16.517857142857142\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 16.015625\n",
            "  Batch    80  of    209.    Elapsed: 0:01:48.\n",
            "(90) train_accuracy : 15.625\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 15.9375\n",
            "  Batch   100  of    209.    Elapsed: 0:02:15.\n",
            "(110) train_accuracy : 17.329545454545453\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 17.96875\n",
            "  Batch   120  of    209.    Elapsed: 0:02:42.\n",
            "(130) train_accuracy : 17.78846153846154\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 16.964285714285715\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 16.875\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 17.1875\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 16.727941176470587\n",
            "  Batch   170  of    209.    Elapsed: 0:03:50.\n",
            "(180) train_accuracy : 16.84027777777778\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 16.94078947368421\n",
            "  Batch   190  of    209.    Elapsed: 0:04:17.\n",
            "(200) train_accuracy : 17.34375\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 31 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 32 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 6.25\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 12.5\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 13.541666666666666\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 13.28125\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 13.125\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 13.020833333333334\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 12.946428571428571\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 12.5\n",
            "  Batch    80  of    209.    Elapsed: 0:01:48.\n",
            "(90) train_accuracy : 13.88888888888889\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 13.75\n",
            "  Batch   100  of    209.    Elapsed: 0:02:15.\n",
            "(110) train_accuracy : 14.488636363636363\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 14.322916666666666\n",
            "  Batch   120  of    209.    Elapsed: 0:02:42.\n",
            "(130) train_accuracy : 14.423076923076923\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 14.732142857142858\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 14.583333333333334\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 14.84375\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 14.338235294117647\n",
            "  Batch   170  of    209.    Elapsed: 0:03:50.\n",
            "(180) train_accuracy : 14.756944444444445\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 15.131578947368421\n",
            "  Batch   190  of    209.    Elapsed: 0:04:17.\n",
            "(200) train_accuracy : 15.625\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 32 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 33 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 12.5\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 18.75\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 18.75\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 17.96875\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 18.125\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 17.1875\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 16.964285714285715\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 15.625\n",
            "  Batch    80  of    209.    Elapsed: 0:01:48.\n",
            "(90) train_accuracy : 15.625\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 14.6875\n",
            "  Batch   100  of    209.    Elapsed: 0:02:15.\n",
            "(110) train_accuracy : 13.920454545454545\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 13.802083333333334\n",
            "  Batch   120  of    209.    Elapsed: 0:02:42.\n",
            "(130) train_accuracy : 13.942307692307692\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 14.0625\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 13.958333333333334\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 13.4765625\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 13.786764705882353\n",
            "  Batch   170  of    209.    Elapsed: 0:03:50.\n",
            "(180) train_accuracy : 13.715277777777779\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 14.30921052631579\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 14.0625\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 33 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 34 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 15.625\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 15.625\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 14.583333333333334\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 14.0625\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 13.75\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 13.020833333333334\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 12.053571428571429\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 12.890625\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 12.152777777777779\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 12.8125\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 13.068181818181818\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 13.020833333333334\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 13.221153846153847\n",
            "  Batch   130  of    209.    Elapsed: 0:02:57.\n",
            "(140) train_accuracy : 13.392857142857142\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 13.333333333333334\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 13.28125\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 13.051470588235293\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 12.5\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 13.322368421052632\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 13.125\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 34 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "244 / 1644 * 100 = 14.841849148418493 \n",
            "\n",
            "======== Epoch 35 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 18.75\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 15.625\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 16.666666666666668\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 16.40625\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 15.0\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 14.0625\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 12.946428571428571\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 13.671875\n",
            "  Batch    80  of    209.    Elapsed: 0:01:48.\n",
            "(90) train_accuracy : 12.847222222222221\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 13.125\n",
            "  Batch   100  of    209.    Elapsed: 0:02:15.\n",
            "(110) train_accuracy : 13.636363636363637\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 13.541666666666666\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 13.942307692307692\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 13.839285714285714\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 13.75\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 14.2578125\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 14.154411764705882\n",
            "  Batch   170  of    209.    Elapsed: 0:03:50.\n",
            "(180) train_accuracy : 13.715277777777779\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 13.486842105263158\n",
            "  Batch   190  of    209.    Elapsed: 0:04:17.\n",
            "(200) train_accuracy : 14.0625\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 35 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "244 / 1644 * 100 = 14.841849148418493 \n",
            "\n",
            "======== Epoch 36 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 15.625\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 20.3125\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 18.75\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 19.53125\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 20.0\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 18.75\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 18.303571428571427\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 17.578125\n",
            "  Batch    80  of    209.    Elapsed: 0:01:48.\n",
            "(90) train_accuracy : 16.319444444444443\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 16.5625\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 16.477272727272727\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 16.40625\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 16.346153846153847\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 16.741071428571427\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 17.083333333333332\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 16.6015625\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 16.544117647058822\n",
            "  Batch   170  of    209.    Elapsed: 0:03:50.\n",
            "(180) train_accuracy : 15.79861111111111\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 15.789473684210526\n",
            "  Batch   190  of    209.    Elapsed: 0:04:17.\n",
            "(200) train_accuracy : 15.46875\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 36 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "244 / 1644 * 100 = 14.841849148418493 \n",
            "\n",
            "======== Epoch 37 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 6.25\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 14.0625\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 15.625\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 14.0625\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 15.0\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 14.583333333333334\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 15.178571428571429\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 14.453125\n",
            "  Batch    80  of    209.    Elapsed: 0:01:48.\n",
            "(90) train_accuracy : 13.88888888888889\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 14.375\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 13.636363636363637\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 14.84375\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 14.903846153846153\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 15.178571428571429\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 15.208333333333334\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 15.0390625\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 14.705882352941176\n",
            "  Batch   170  of    209.    Elapsed: 0:03:50.\n",
            "(180) train_accuracy : 14.756944444444445\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 15.296052631578947\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 14.84375\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 37 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "244 / 1644 * 100 = 14.841849148418493 \n",
            "\n",
            "======== Epoch 38 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 15.625\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 15.625\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 13.541666666666666\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 14.84375\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 13.125\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 14.583333333333334\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 13.839285714285714\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 14.453125\n",
            "  Batch    80  of    209.    Elapsed: 0:01:48.\n",
            "(90) train_accuracy : 15.277777777777779\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 15.625\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 15.340909090909092\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 14.583333333333334\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 14.903846153846153\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 15.625\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 15.625\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 16.2109375\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 16.727941176470587\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 16.666666666666668\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 16.282894736842106\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 16.25\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 38 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "244 / 1644 * 100 = 14.841849148418493 \n",
            "\n",
            "======== Epoch 39 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 25.0\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 17.1875\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 16.666666666666668\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 14.84375\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 16.25\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 15.625\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 15.625\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 14.0625\n",
            "  Batch    80  of    209.    Elapsed: 0:01:48.\n",
            "(90) train_accuracy : 13.194444444444445\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 13.125\n",
            "  Batch   100  of    209.    Elapsed: 0:02:15.\n",
            "(110) train_accuracy : 12.784090909090908\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 12.760416666666666\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 12.5\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 12.723214285714286\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 13.541666666666666\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 13.671875\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 14.705882352941176\n",
            "  Batch   170  of    209.    Elapsed: 0:03:50.\n",
            "(180) train_accuracy : 14.583333333333334\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 14.473684210526315\n",
            "  Batch   190  of    209.    Elapsed: 0:04:17.\n",
            "(200) train_accuracy : 14.84375\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 39 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "227 / 1644 * 100 = 13.80778588807786 \n",
            "\n",
            "======== Epoch 40 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 18.75\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 14.0625\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 15.625\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 17.1875\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 15.625\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 16.145833333333332\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 16.964285714285715\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 16.40625\n",
            "  Batch    80  of    209.    Elapsed: 0:01:48.\n",
            "(90) train_accuracy : 16.319444444444443\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 16.25\n",
            "  Batch   100  of    209.    Elapsed: 0:02:15.\n",
            "(110) train_accuracy : 16.193181818181817\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 15.625\n",
            "  Batch   120  of    209.    Elapsed: 0:02:42.\n",
            "(130) train_accuracy : 16.346153846153847\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 15.848214285714286\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 15.416666666666666\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 15.4296875\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 15.441176470588236\n",
            "  Batch   170  of    209.    Elapsed: 0:03:50.\n",
            "(180) train_accuracy : 15.79861111111111\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 15.460526315789474\n",
            "  Batch   190  of    209.    Elapsed: 0:04:17.\n",
            "(200) train_accuracy : 15.46875\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 40 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 41 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 6.25\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 12.5\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 12.5\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 14.0625\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 15.0\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 14.0625\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 12.946428571428571\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 13.671875\n",
            "  Batch    80  of    209.    Elapsed: 0:01:48.\n",
            "(90) train_accuracy : 15.277777777777779\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 14.375\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 14.488636363636363\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 14.0625\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 14.663461538461538\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 14.0625\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 13.541666666666666\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 14.0625\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 14.154411764705882\n",
            "  Batch   170  of    209.    Elapsed: 0:03:50.\n",
            "(180) train_accuracy : 14.409722222222221\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 14.473684210526315\n",
            "  Batch   190  of    209.    Elapsed: 0:04:17.\n",
            "(200) train_accuracy : 14.53125\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 41 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "217 / 1644 * 100 = 13.199513381995134 \n",
            "\n",
            "======== Epoch 42 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 12.5\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 12.5\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 17.708333333333332\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 19.53125\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 17.5\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 17.1875\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 17.410714285714285\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 17.578125\n",
            "  Batch    80  of    209.    Elapsed: 0:01:48.\n",
            "(90) train_accuracy : 16.319444444444443\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 15.9375\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 16.477272727272727\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 16.666666666666668\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 16.10576923076923\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 15.625\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 15.0\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 15.0390625\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 15.257352941176471\n",
            "  Batch   170  of    209.    Elapsed: 0:03:50.\n",
            "(180) train_accuracy : 14.756944444444445\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 14.802631578947368\n",
            "  Batch   190  of    209.    Elapsed: 0:04:17.\n",
            "(200) train_accuracy : 14.375\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 42 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "209 / 1644 * 100 = 12.712895377128955 \n",
            "\n",
            "======== Epoch 43 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 12.5\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 18.75\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 18.75\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 15.625\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 15.625\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 16.666666666666668\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 18.303571428571427\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 17.578125\n",
            "  Batch    80  of    209.    Elapsed: 0:01:48.\n",
            "(90) train_accuracy : 17.01388888888889\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 16.25\n",
            "  Batch   100  of    209.    Elapsed: 0:02:15.\n",
            "(110) train_accuracy : 16.193181818181817\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 15.885416666666666\n",
            "  Batch   120  of    209.    Elapsed: 0:02:42.\n",
            "(130) train_accuracy : 16.10576923076923\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 15.178571428571429\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 15.0\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 14.84375\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 14.522058823529411\n",
            "  Batch   170  of    209.    Elapsed: 0:03:50.\n",
            "(180) train_accuracy : 13.88888888888889\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 14.473684210526315\n",
            "  Batch   190  of    209.    Elapsed: 0:04:17.\n",
            "(200) train_accuracy : 14.6875\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 43 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "217 / 1644 * 100 = 13.199513381995134 \n",
            "\n",
            "======== Epoch 44 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 21.875\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 15.625\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 13.541666666666666\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 13.28125\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 16.25\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 16.666666666666668\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 17.410714285714285\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 18.359375\n",
            "  Batch    80  of    209.    Elapsed: 0:01:48.\n",
            "(90) train_accuracy : 18.055555555555557\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 17.5\n",
            "  Batch   100  of    209.    Elapsed: 0:02:15.\n",
            "(110) train_accuracy : 16.761363636363637\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 17.447916666666668\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 17.067307692307693\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 16.964285714285715\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 16.458333333333332\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 16.9921875\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 17.279411764705884\n",
            "  Batch   170  of    209.    Elapsed: 0:03:50.\n",
            "(180) train_accuracy : 17.36111111111111\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 17.269736842105264\n",
            "  Batch   190  of    209.    Elapsed: 0:04:17.\n",
            "(200) train_accuracy : 17.34375\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 44 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 45 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 21.875\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 15.625\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 16.666666666666668\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 14.84375\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 15.0\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 14.583333333333334\n",
            "  Batch    60  of    209.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 14.732142857142858\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 13.28125\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 13.194444444444445\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 14.0625\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 13.068181818181818\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 12.760416666666666\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 13.701923076923077\n",
            "  Batch   130  of    209.    Elapsed: 0:02:57.\n",
            "(140) train_accuracy : 14.0625\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 14.375\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 14.2578125\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 14.522058823529411\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 14.583333333333334\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 14.802631578947368\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 14.6875\n",
            "  Batch   200  of    209.    Elapsed: 0:04:32.\n",
            "Epoch 45 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "244 / 1644 * 100 = 14.841849148418493 \n",
            "\n",
            "======== Epoch 46 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 21.875\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 18.75\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 16.666666666666668\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 17.1875\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 16.25\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 17.708333333333332\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 16.517857142857142\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 16.796875\n",
            "  Batch    80  of    209.    Elapsed: 0:01:48.\n",
            "(90) train_accuracy : 17.36111111111111\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 17.1875\n",
            "  Batch   100  of    209.    Elapsed: 0:02:15.\n",
            "(110) train_accuracy : 16.193181818181817\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 16.666666666666668\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 16.346153846153847\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 15.401785714285714\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 15.625\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 15.625\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 15.808823529411764\n",
            "  Batch   170  of    209.    Elapsed: 0:03:50.\n",
            "(180) train_accuracy : 15.625\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 15.625\n",
            "  Batch   190  of    209.    Elapsed: 0:04:17.\n",
            "(200) train_accuracy : 15.15625\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 46 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 47 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 18.75\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 18.75\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 17.708333333333332\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 17.96875\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 17.5\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 16.666666666666668\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 16.071428571428573\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 15.234375\n",
            "  Batch    80  of    209.    Elapsed: 0:01:48.\n",
            "(90) train_accuracy : 16.319444444444443\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 16.5625\n",
            "  Batch   100  of    209.    Elapsed: 0:02:15.\n",
            "(110) train_accuracy : 17.613636363636363\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 16.927083333333332\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 17.307692307692307\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 17.410714285714285\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 17.291666666666668\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 17.578125\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 17.647058823529413\n",
            "  Batch   170  of    209.    Elapsed: 0:03:50.\n",
            "(180) train_accuracy : 17.36111111111111\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 16.611842105263158\n",
            "  Batch   190  of    209.    Elapsed: 0:04:17.\n",
            "(200) train_accuracy : 17.03125\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 47 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "227 / 1644 * 100 = 13.80778588807786 \n",
            "\n",
            "======== Epoch 48 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 9.375\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 12.5\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 13.541666666666666\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 12.5\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 10.625\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 10.416666666666666\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 11.160714285714286\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 11.328125\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 11.458333333333334\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 11.875\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 12.5\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 12.760416666666666\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 12.740384615384615\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 12.946428571428571\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 12.708333333333334\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 12.890625\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 12.316176470588236\n",
            "  Batch   170  of    209.    Elapsed: 0:03:50.\n",
            "(180) train_accuracy : 11.979166666666666\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 12.664473684210526\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 12.65625\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 48 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 49 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 15.625\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 15.625\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 14.583333333333334\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 17.1875\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 15.625\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 15.104166666666666\n",
            "  Batch    60  of    209.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 15.625\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 15.625\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 16.666666666666668\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 16.25\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 16.761363636363637\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 16.40625\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 16.826923076923077\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 16.071428571428573\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 16.041666666666668\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 15.625\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 15.625\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 15.45138888888889\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 14.802631578947368\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 15.3125\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 49 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "217 / 1644 * 100 = 13.199513381995134 \n",
            "\n",
            "======== Epoch 50 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 15.625\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 18.75\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 13.541666666666666\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 14.0625\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 14.375\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 14.0625\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 12.946428571428571\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 11.71875\n",
            "  Batch    80  of    209.    Elapsed: 0:01:48.\n",
            "(90) train_accuracy : 12.847222222222221\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 13.125\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 13.352272727272727\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 14.0625\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 14.423076923076923\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 14.285714285714286\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 13.75\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 13.28125\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 13.051470588235293\n",
            "  Batch   170  of    209.    Elapsed: 0:03:50.\n",
            "(180) train_accuracy : 12.847222222222221\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 12.664473684210526\n",
            "  Batch   190  of    209.    Elapsed: 0:04:17.\n",
            "(200) train_accuracy : 12.65625\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 50 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 51 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 15.625\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 17.1875\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 15.625\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 14.0625\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 11.875\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 15.104166666666666\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 14.285714285714286\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 15.234375\n",
            "  Batch    80  of    209.    Elapsed: 0:01:48.\n",
            "(90) train_accuracy : 14.930555555555555\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 15.3125\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 15.625\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 14.84375\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 14.663461538461538\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 14.955357142857142\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 15.208333333333334\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 15.4296875\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 15.257352941176471\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 14.409722222222221\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 13.81578947368421\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 13.75\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 51 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "244 / 1644 * 100 = 14.841849148418493 \n",
            "\n",
            "======== Epoch 52 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 9.375\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 9.375\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 12.5\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 12.5\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 13.75\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 14.583333333333334\n",
            "  Batch    60  of    209.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 15.178571428571429\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 13.28125\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 13.88888888888889\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 14.0625\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 14.488636363636363\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 14.84375\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 15.14423076923077\n",
            "  Batch   130  of    209.    Elapsed: 0:02:57.\n",
            "(140) train_accuracy : 15.401785714285714\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 15.208333333333334\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 15.4296875\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 15.992647058823529\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 15.79861111111111\n",
            "  Batch   180  of    209.    Elapsed: 0:04:05.\n",
            "(190) train_accuracy : 15.296052631578947\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 14.84375\n",
            "  Batch   200  of    209.    Elapsed: 0:04:32.\n",
            "Epoch 52 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "244 / 1644 * 100 = 14.841849148418493 \n",
            "\n",
            "======== Epoch 53 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 9.375\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 12.5\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 13.541666666666666\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 12.5\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 13.75\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 15.625\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 14.732142857142858\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 14.84375\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 15.625\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 15.0\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 15.056818181818182\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 14.322916666666666\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 14.663461538461538\n",
            "  Batch   130  of    209.    Elapsed: 0:02:57.\n",
            "(140) train_accuracy : 14.732142857142858\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 14.166666666666666\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 14.0625\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 13.970588235294118\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 14.0625\n",
            "  Batch   180  of    209.    Elapsed: 0:04:05.\n",
            "(190) train_accuracy : 13.980263157894736\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 14.375\n",
            "  Batch   200  of    209.    Elapsed: 0:04:32.\n",
            "Epoch 53 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "244 / 1644 * 100 = 14.841849148418493 \n",
            "\n",
            "======== Epoch 54 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 6.25\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 7.8125\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 10.416666666666666\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 10.9375\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 12.5\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 15.625\n",
            "  Batch    60  of    209.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 14.732142857142858\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 13.671875\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 13.194444444444445\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 14.375\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 14.488636363636363\n",
            "  Batch   110  of    209.    Elapsed: 0:02:30.\n",
            "(120) train_accuracy : 14.583333333333334\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 13.942307692307692\n",
            "  Batch   130  of    209.    Elapsed: 0:02:57.\n",
            "(140) train_accuracy : 13.169642857142858\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 13.541666666666666\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 14.453125\n",
            "  Batch   160  of    209.    Elapsed: 0:03:38.\n",
            "(170) train_accuracy : 14.705882352941176\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 14.583333333333334\n",
            "  Batch   180  of    209.    Elapsed: 0:04:05.\n",
            "(190) train_accuracy : 14.967105263157896\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 14.84375\n",
            "  Batch   200  of    209.    Elapsed: 0:04:32.\n",
            "Epoch 54 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 55 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 18.75\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 17.1875\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 18.75\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 17.96875\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 17.5\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 16.145833333333332\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 17.857142857142858\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 17.578125\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 17.708333333333332\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 18.125\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 17.897727272727273\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 17.447916666666668\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 16.58653846153846\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 15.401785714285714\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 15.0\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 14.84375\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 14.522058823529411\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 14.409722222222221\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 14.473684210526315\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 14.53125\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 55 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "244 / 1644 * 100 = 14.841849148418493 \n",
            "\n",
            "======== Epoch 56 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 9.375\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 12.5\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 13.541666666666666\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 14.0625\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 13.125\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 14.0625\n",
            "  Batch    60  of    209.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 13.839285714285714\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 12.890625\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 12.847222222222221\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 13.125\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 12.5\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 13.020833333333334\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 13.461538461538462\n",
            "  Batch   130  of    209.    Elapsed: 0:02:57.\n",
            "(140) train_accuracy : 13.392857142857142\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 13.541666666666666\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 13.28125\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 13.051470588235293\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 13.194444444444445\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 13.486842105263158\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 13.59375\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 56 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "55     56  14.841849\n",
            "244 / 1644 * 100 = 14.841849148418493 \n",
            "\n",
            "======== Epoch 57 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 6.25\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 6.25\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 13.541666666666666\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 15.625\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 15.0\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 14.0625\n",
            "  Batch    60  of    209.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 12.946428571428571\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 12.890625\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 13.541666666666666\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 13.125\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 13.068181818181818\n",
            "  Batch   110  of    209.    Elapsed: 0:02:30.\n",
            "(120) train_accuracy : 13.802083333333334\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 13.461538461538462\n",
            "  Batch   130  of    209.    Elapsed: 0:02:57.\n",
            "(140) train_accuracy : 13.616071428571429\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 13.125\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 13.0859375\n",
            "  Batch   160  of    209.    Elapsed: 0:03:38.\n",
            "(170) train_accuracy : 12.867647058823529\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 13.368055555555555\n",
            "  Batch   180  of    209.    Elapsed: 0:04:05.\n",
            "(190) train_accuracy : 13.980263157894736\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 13.59375\n",
            "  Batch   200  of    209.    Elapsed: 0:04:32.\n",
            "Epoch 57 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "55     56  14.841849\n",
            "56     57  14.841849\n",
            "244 / 1644 * 100 = 14.841849148418493 \n",
            "\n",
            "======== Epoch 58 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 9.375\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 15.625\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 13.541666666666666\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 11.71875\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 11.25\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 11.979166666666666\n",
            "  Batch    60  of    209.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 10.714285714285714\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 11.71875\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 10.76388888888889\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 10.9375\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 10.795454545454545\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 10.677083333333334\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 10.817307692307692\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 10.9375\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 10.416666666666666\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 10.15625\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 10.477941176470589\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 11.11111111111111\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 11.348684210526315\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 11.5625\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 58 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "55     56  14.841849\n",
            "56     57  14.841849\n",
            "57     58  14.841849\n",
            "244 / 1644 * 100 = 14.841849148418493 \n",
            "\n",
            "======== Epoch 59 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 21.875\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 17.1875\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 21.875\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 18.75\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 16.25\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 15.625\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 15.178571428571429\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 14.453125\n",
            "  Batch    80  of    209.    Elapsed: 0:01:48.\n",
            "(90) train_accuracy : 15.625\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 15.0\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 14.204545454545455\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 14.322916666666666\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 14.182692307692308\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 14.508928571428571\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 15.0\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 14.6484375\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 14.154411764705882\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 13.88888888888889\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 14.144736842105264\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 14.21875\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 59 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "55     56  14.841849\n",
            "56     57  14.841849\n",
            "57     58  14.841849\n",
            "58     59  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 60 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 15.625\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 12.5\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 11.458333333333334\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 11.71875\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 11.25\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 11.458333333333334\n",
            "  Batch    60  of    209.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 10.714285714285714\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 10.9375\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 11.805555555555555\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 12.5\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 12.215909090909092\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 12.239583333333334\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 11.778846153846153\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 11.160714285714286\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 12.291666666666666\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 12.109375\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 12.5\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 12.32638888888889\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 12.171052631578947\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 12.65625\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 60 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "55     56  14.841849\n",
            "56     57  14.841849\n",
            "57     58  14.841849\n",
            "58     59  15.571776\n",
            "59     60  14.841849\n",
            "244 / 1644 * 100 = 14.841849148418493 \n",
            "\n",
            "======== Epoch 61 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 12.5\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 17.1875\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 15.625\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 17.1875\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 15.0\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 15.104166666666666\n",
            "  Batch    60  of    209.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 15.178571428571429\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 14.453125\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 15.277777777777779\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 14.375\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 13.920454545454545\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 14.84375\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 15.14423076923077\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 15.625\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 16.041666666666668\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 15.8203125\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 15.808823529411764\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 15.625\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 15.625\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 15.625\n",
            "  Batch   200  of    209.    Elapsed: 0:04:32.\n",
            "Epoch 61 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "55     56  14.841849\n",
            "56     57  14.841849\n",
            "57     58  14.841849\n",
            "58     59  15.571776\n",
            "59     60  14.841849\n",
            "60     61  13.807786\n",
            "227 / 1644 * 100 = 13.80778588807786 \n",
            "\n",
            "======== Epoch 62 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 6.25\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 7.8125\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 10.416666666666666\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 13.28125\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 15.0\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 15.104166666666666\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 15.178571428571429\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 14.453125\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 13.194444444444445\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 12.1875\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 12.215909090909092\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 11.71875\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 11.298076923076923\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 11.607142857142858\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 11.041666666666666\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 10.7421875\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 11.397058823529411\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 11.805555555555555\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 11.842105263157896\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 12.1875\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 62 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "55     56  14.841849\n",
            "56     57  14.841849\n",
            "57     58  14.841849\n",
            "58     59  15.571776\n",
            "59     60  14.841849\n",
            "60     61  13.807786\n",
            "61     62  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 63 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 15.625\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 14.0625\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 13.541666666666666\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 12.5\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 12.5\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 11.979166666666666\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 12.946428571428571\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 13.28125\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 13.194444444444445\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 12.8125\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 12.784090909090908\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 13.541666666666666\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 13.221153846153847\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 13.169642857142858\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 13.333333333333334\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 13.8671875\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 14.522058823529411\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 14.756944444444445\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 14.638157894736842\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 15.15625\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 63 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "55     56  14.841849\n",
            "56     57  14.841849\n",
            "57     58  14.841849\n",
            "58     59  15.571776\n",
            "59     60  14.841849\n",
            "60     61  13.807786\n",
            "61     62  15.571776\n",
            "62     63  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 64 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 25.0\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 23.4375\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 20.833333333333332\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 18.75\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 19.375\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 19.270833333333332\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 18.75\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 17.1875\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 17.01388888888889\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 16.875\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 16.477272727272727\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 16.927083333333332\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 16.346153846153847\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 15.625\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 15.625\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 15.234375\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 14.889705882352942\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 14.409722222222221\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 14.473684210526315\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 14.21875\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 64 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "55     56  14.841849\n",
            "56     57  14.841849\n",
            "57     58  14.841849\n",
            "58     59  15.571776\n",
            "59     60  14.841849\n",
            "60     61  13.807786\n",
            "61     62  15.571776\n",
            "62     63  15.571776\n",
            "63     64  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 65 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 9.375\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 10.9375\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 12.5\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 14.0625\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 13.125\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 12.5\n",
            "  Batch    60  of    209.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 12.946428571428571\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 13.671875\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 13.541666666666666\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 13.75\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 13.920454545454545\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 14.0625\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 14.423076923076923\n",
            "  Batch   130  of    209.    Elapsed: 0:02:57.\n",
            "(140) train_accuracy : 14.285714285714286\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 14.375\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 15.234375\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 16.360294117647058\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 16.319444444444443\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 15.789473684210526\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 16.40625\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 65 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "55     56  14.841849\n",
            "56     57  14.841849\n",
            "57     58  14.841849\n",
            "58     59  15.571776\n",
            "59     60  14.841849\n",
            "60     61  13.807786\n",
            "61     62  15.571776\n",
            "62     63  15.571776\n",
            "63     64  15.571776\n",
            "64     65  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 66 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 18.75\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 17.1875\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 18.75\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 17.96875\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 19.375\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 19.270833333333332\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 19.196428571428573\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 19.921875\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 18.75\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 18.75\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 17.897727272727273\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 17.447916666666668\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 18.509615384615383\n",
            "  Batch   130  of    209.    Elapsed: 0:02:57.\n",
            "(140) train_accuracy : 18.080357142857142\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 17.708333333333332\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 17.3828125\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 17.095588235294116\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 16.666666666666668\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 16.11842105263158\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 16.09375\n",
            "  Batch   200  of    209.    Elapsed: 0:04:32.\n",
            "Epoch 66 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "55     56  14.841849\n",
            "56     57  14.841849\n",
            "57     58  14.841849\n",
            "58     59  15.571776\n",
            "59     60  14.841849\n",
            "60     61  13.807786\n",
            "61     62  15.571776\n",
            "62     63  15.571776\n",
            "63     64  15.571776\n",
            "64     65  15.571776\n",
            "65     66  14.841849\n",
            "244 / 1644 * 100 = 14.841849148418493 \n",
            "\n",
            "======== Epoch 67 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 0.0\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 9.375\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 8.333333333333334\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 10.9375\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 12.5\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 13.020833333333334\n",
            "  Batch    60  of    209.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 12.5\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 13.671875\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 13.88888888888889\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 13.4375\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 13.352272727272727\n",
            "  Batch   110  of    209.    Elapsed: 0:02:30.\n",
            "(120) train_accuracy : 14.0625\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 14.182692307692308\n",
            "  Batch   130  of    209.    Elapsed: 0:02:57.\n",
            "(140) train_accuracy : 14.0625\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 13.541666666666666\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 13.8671875\n",
            "  Batch   160  of    209.    Elapsed: 0:03:38.\n",
            "(170) train_accuracy : 13.970588235294118\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 13.88888888888889\n",
            "  Batch   180  of    209.    Elapsed: 0:04:05.\n",
            "(190) train_accuracy : 13.980263157894736\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 14.0625\n",
            "  Batch   200  of    209.    Elapsed: 0:04:32.\n",
            "Epoch 67 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "55     56  14.841849\n",
            "56     57  14.841849\n",
            "57     58  14.841849\n",
            "58     59  15.571776\n",
            "59     60  14.841849\n",
            "60     61  13.807786\n",
            "61     62  15.571776\n",
            "62     63  15.571776\n",
            "63     64  15.571776\n",
            "64     65  15.571776\n",
            "65     66  14.841849\n",
            "66     67  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 68 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 18.75\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 14.0625\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 15.625\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 17.96875\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 17.5\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 18.75\n",
            "  Batch    60  of    209.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 16.964285714285715\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 16.40625\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 15.277777777777779\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 15.625\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 15.340909090909092\n",
            "  Batch   110  of    209.    Elapsed: 0:02:30.\n",
            "(120) train_accuracy : 15.625\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 14.903846153846153\n",
            "  Batch   130  of    209.    Elapsed: 0:02:57.\n",
            "(140) train_accuracy : 14.508928571428571\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 14.583333333333334\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 14.6484375\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 15.073529411764707\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 14.930555555555555\n",
            "  Batch   180  of    209.    Elapsed: 0:04:05.\n",
            "(190) train_accuracy : 14.802631578947368\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 15.0\n",
            "  Batch   200  of    209.    Elapsed: 0:04:32.\n",
            "Epoch 68 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "55     56  14.841849\n",
            "56     57  14.841849\n",
            "57     58  14.841849\n",
            "58     59  15.571776\n",
            "59     60  14.841849\n",
            "60     61  13.807786\n",
            "61     62  15.571776\n",
            "62     63  15.571776\n",
            "63     64  15.571776\n",
            "64     65  15.571776\n",
            "65     66  14.841849\n",
            "66     67  15.571776\n",
            "67     68  13.807786\n",
            "227 / 1644 * 100 = 13.80778588807786 \n",
            "\n",
            "======== Epoch 69 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 12.5\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 18.75\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 14.583333333333334\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 13.28125\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 13.75\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 14.583333333333334\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 13.839285714285714\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 13.28125\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 13.541666666666666\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 14.0625\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 14.488636363636363\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 14.322916666666666\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 13.942307692307692\n",
            "  Batch   130  of    209.    Elapsed: 0:02:57.\n",
            "(140) train_accuracy : 13.616071428571429\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 14.583333333333334\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 14.453125\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 14.705882352941176\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 14.23611111111111\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 14.802631578947368\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 14.6875\n",
            "  Batch   200  of    209.    Elapsed: 0:04:32.\n",
            "Epoch 69 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "55     56  14.841849\n",
            "56     57  14.841849\n",
            "57     58  14.841849\n",
            "58     59  15.571776\n",
            "59     60  14.841849\n",
            "60     61  13.807786\n",
            "61     62  15.571776\n",
            "62     63  15.571776\n",
            "63     64  15.571776\n",
            "64     65  15.571776\n",
            "65     66  14.841849\n",
            "66     67  15.571776\n",
            "67     68  13.807786\n",
            "68     69  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 70 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 21.875\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 20.3125\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 19.791666666666668\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 20.3125\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 19.375\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 18.229166666666668\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 17.410714285714285\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 16.40625\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 16.319444444444443\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 16.25\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 15.625\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 16.145833333333332\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 15.625\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 16.517857142857142\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 15.833333333333334\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 15.4296875\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 15.441176470588236\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 15.625\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 15.789473684210526\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 15.0\n",
            "  Batch   200  of    209.    Elapsed: 0:04:32.\n",
            "Epoch 70 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "55     56  14.841849\n",
            "56     57  14.841849\n",
            "57     58  14.841849\n",
            "58     59  15.571776\n",
            "59     60  14.841849\n",
            "60     61  13.807786\n",
            "61     62  15.571776\n",
            "62     63  15.571776\n",
            "63     64  15.571776\n",
            "64     65  15.571776\n",
            "65     66  14.841849\n",
            "66     67  15.571776\n",
            "67     68  13.807786\n",
            "68     69  15.571776\n",
            "69     70  14.841849\n",
            "244 / 1644 * 100 = 14.841849148418493 \n",
            "\n",
            "======== Epoch 71 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 12.5\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 14.0625\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 14.583333333333334\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 16.40625\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 15.625\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 17.708333333333332\n",
            "  Batch    60  of    209.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 16.071428571428573\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 16.015625\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 16.666666666666668\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 16.25\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 15.909090909090908\n",
            "  Batch   110  of    209.    Elapsed: 0:02:30.\n",
            "(120) train_accuracy : 15.625\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 15.865384615384615\n",
            "  Batch   130  of    209.    Elapsed: 0:02:57.\n",
            "(140) train_accuracy : 15.401785714285714\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 15.416666666666666\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 15.234375\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 15.257352941176471\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 15.277777777777779\n",
            "  Batch   180  of    209.    Elapsed: 0:04:05.\n",
            "(190) train_accuracy : 15.625\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 15.78125\n",
            "  Batch   200  of    209.    Elapsed: 0:04:32.\n",
            "Epoch 71 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "55     56  14.841849\n",
            "56     57  14.841849\n",
            "57     58  14.841849\n",
            "58     59  15.571776\n",
            "59     60  14.841849\n",
            "60     61  13.807786\n",
            "61     62  15.571776\n",
            "62     63  15.571776\n",
            "63     64  15.571776\n",
            "64     65  15.571776\n",
            "65     66  14.841849\n",
            "66     67  15.571776\n",
            "67     68  13.807786\n",
            "68     69  15.571776\n",
            "69     70  14.841849\n",
            "70     71  12.712895\n",
            "209 / 1644 * 100 = 12.712895377128955 \n",
            "\n",
            "======== Epoch 72 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 18.75\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 14.0625\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 14.583333333333334\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 12.5\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 15.625\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 14.0625\n",
            "  Batch    60  of    209.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 15.625\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 14.0625\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 14.583333333333334\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 14.0625\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 15.056818181818182\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 14.322916666666666\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 14.182692307692308\n",
            "  Batch   130  of    209.    Elapsed: 0:02:57.\n",
            "(140) train_accuracy : 14.285714285714286\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 14.375\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 13.671875\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 13.419117647058824\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 13.541666666666666\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 13.486842105263158\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 12.96875\n",
            "  Batch   200  of    209.    Elapsed: 0:04:32.\n",
            "Epoch 72 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "55     56  14.841849\n",
            "56     57  14.841849\n",
            "57     58  14.841849\n",
            "58     59  15.571776\n",
            "59     60  14.841849\n",
            "60     61  13.807786\n",
            "61     62  15.571776\n",
            "62     63  15.571776\n",
            "63     64  15.571776\n",
            "64     65  15.571776\n",
            "65     66  14.841849\n",
            "66     67  15.571776\n",
            "67     68  13.807786\n",
            "68     69  15.571776\n",
            "69     70  14.841849\n",
            "70     71  12.712895\n",
            "71     72  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 73 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 12.5\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 20.3125\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 21.875\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 21.875\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 21.25\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 21.875\n",
            "  Batch    60  of    209.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 21.428571428571427\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 21.484375\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 20.13888888888889\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 20.0\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 19.318181818181817\n",
            "  Batch   110  of    209.    Elapsed: 0:02:30.\n",
            "(120) train_accuracy : 18.75\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 18.509615384615383\n",
            "  Batch   130  of    209.    Elapsed: 0:02:57.\n",
            "(140) train_accuracy : 18.080357142857142\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 18.125\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 17.7734375\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 17.647058823529413\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 17.53472222222222\n",
            "  Batch   180  of    209.    Elapsed: 0:04:05.\n",
            "(190) train_accuracy : 17.763157894736842\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 18.125\n",
            "  Batch   200  of    209.    Elapsed: 0:04:32.\n",
            "Epoch 73 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "55     56  14.841849\n",
            "56     57  14.841849\n",
            "57     58  14.841849\n",
            "58     59  15.571776\n",
            "59     60  14.841849\n",
            "60     61  13.807786\n",
            "61     62  15.571776\n",
            "62     63  15.571776\n",
            "63     64  15.571776\n",
            "64     65  15.571776\n",
            "65     66  14.841849\n",
            "66     67  15.571776\n",
            "67     68  13.807786\n",
            "68     69  15.571776\n",
            "69     70  14.841849\n",
            "70     71  12.712895\n",
            "71     72  15.571776\n",
            "72     73  14.841849\n",
            "244 / 1644 * 100 = 14.841849148418493 \n",
            "\n",
            "======== Epoch 74 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 12.5\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 15.625\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 14.583333333333334\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 12.5\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 13.75\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 11.979166666666666\n",
            "  Batch    60  of    209.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 11.160714285714286\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 11.71875\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 12.5\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 12.8125\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 12.784090909090908\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 13.802083333333334\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 13.701923076923077\n",
            "  Batch   130  of    209.    Elapsed: 0:02:57.\n",
            "(140) train_accuracy : 13.839285714285714\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 13.75\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 13.4765625\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 13.051470588235293\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 13.368055555555555\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 13.651315789473685\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 13.90625\n",
            "  Batch   200  of    209.    Elapsed: 0:04:32.\n",
            "Epoch 74 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "55     56  14.841849\n",
            "56     57  14.841849\n",
            "57     58  14.841849\n",
            "58     59  15.571776\n",
            "59     60  14.841849\n",
            "60     61  13.807786\n",
            "61     62  15.571776\n",
            "62     63  15.571776\n",
            "63     64  15.571776\n",
            "64     65  15.571776\n",
            "65     66  14.841849\n",
            "66     67  15.571776\n",
            "67     68  13.807786\n",
            "68     69  15.571776\n",
            "69     70  14.841849\n",
            "70     71  12.712895\n",
            "71     72  15.571776\n",
            "72     73  14.841849\n",
            "73     74  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 75 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 21.875\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 18.75\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 20.833333333333332\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 18.75\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 21.875\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 21.875\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 20.982142857142858\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 20.3125\n",
            "  Batch    80  of    209.    Elapsed: 0:01:48.\n",
            "(90) train_accuracy : 22.569444444444443\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 22.5\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 22.443181818181817\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 21.875\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 22.596153846153847\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 21.651785714285715\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 21.25\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 21.6796875\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 21.323529411764707\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 20.48611111111111\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 20.06578947368421\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 20.0\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 75 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "55     56  14.841849\n",
            "56     57  14.841849\n",
            "57     58  14.841849\n",
            "58     59  15.571776\n",
            "59     60  14.841849\n",
            "60     61  13.807786\n",
            "61     62  15.571776\n",
            "62     63  15.571776\n",
            "63     64  15.571776\n",
            "64     65  15.571776\n",
            "65     66  14.841849\n",
            "66     67  15.571776\n",
            "67     68  13.807786\n",
            "68     69  15.571776\n",
            "69     70  14.841849\n",
            "70     71  12.712895\n",
            "71     72  15.571776\n",
            "72     73  14.841849\n",
            "73     74  15.571776\n",
            "74     75  13.807786\n",
            "227 / 1644 * 100 = 13.80778588807786 \n",
            "\n",
            "======== Epoch 76 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 21.875\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 18.75\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 16.666666666666668\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 15.625\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 15.625\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 16.145833333333332\n",
            "  Batch    60  of    209.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 16.964285714285715\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 15.625\n",
            "  Batch    80  of    209.    Elapsed: 0:01:48.\n",
            "(90) train_accuracy : 15.625\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 15.625\n",
            "  Batch   100  of    209.    Elapsed: 0:02:15.\n",
            "(110) train_accuracy : 15.625\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 15.885416666666666\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 15.384615384615385\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 15.178571428571429\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 14.791666666666666\n",
            "  Batch   150  of    209.    Elapsed: 0:03:23.\n",
            "(160) train_accuracy : 15.0390625\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 14.889705882352942\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 14.756944444444445\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 15.131578947368421\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 15.3125\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 76 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "55     56  14.841849\n",
            "56     57  14.841849\n",
            "57     58  14.841849\n",
            "58     59  15.571776\n",
            "59     60  14.841849\n",
            "60     61  13.807786\n",
            "61     62  15.571776\n",
            "62     63  15.571776\n",
            "63     64  15.571776\n",
            "64     65  15.571776\n",
            "65     66  14.841849\n",
            "66     67  15.571776\n",
            "67     68  13.807786\n",
            "68     69  15.571776\n",
            "69     70  14.841849\n",
            "70     71  12.712895\n",
            "71     72  15.571776\n",
            "72     73  14.841849\n",
            "73     74  15.571776\n",
            "74     75  13.807786\n",
            "75     76  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 77 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 9.375\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 9.375\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 8.333333333333334\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 10.9375\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 8.75\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 8.854166666666666\n",
            "  Batch    60  of    209.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 9.821428571428571\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 11.71875\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 11.805555555555555\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 12.8125\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 11.931818181818182\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 12.239583333333334\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 12.259615384615385\n",
            "  Batch   130  of    209.    Elapsed: 0:02:56.\n",
            "(140) train_accuracy : 11.830357142857142\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 11.875\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 11.71875\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 11.397058823529411\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 11.631944444444445\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 11.18421052631579\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 11.40625\n",
            "  Batch   200  of    209.    Elapsed: 0:04:31.\n",
            "Epoch 77 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "55     56  14.841849\n",
            "56     57  14.841849\n",
            "57     58  14.841849\n",
            "58     59  15.571776\n",
            "59     60  14.841849\n",
            "60     61  13.807786\n",
            "61     62  15.571776\n",
            "62     63  15.571776\n",
            "63     64  15.571776\n",
            "64     65  15.571776\n",
            "65     66  14.841849\n",
            "66     67  15.571776\n",
            "67     68  13.807786\n",
            "68     69  15.571776\n",
            "69     70  14.841849\n",
            "70     71  12.712895\n",
            "71     72  15.571776\n",
            "72     73  14.841849\n",
            "73     74  15.571776\n",
            "74     75  13.807786\n",
            "75     76  15.571776\n",
            "76     77  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 78 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 21.875\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 18.75\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 16.666666666666668\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 14.84375\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 13.75\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 12.5\n",
            "  Batch    60  of    209.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 13.392857142857142\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 12.890625\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 12.5\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 12.8125\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 12.5\n",
            "  Batch   110  of    209.    Elapsed: 0:02:29.\n",
            "(120) train_accuracy : 13.020833333333334\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 12.98076923076923\n",
            "  Batch   130  of    209.    Elapsed: 0:02:57.\n",
            "(140) train_accuracy : 12.946428571428571\n",
            "  Batch   140  of    209.    Elapsed: 0:03:10.\n",
            "(150) train_accuracy : 13.333333333333334\n",
            "  Batch   150  of    209.    Elapsed: 0:03:24.\n",
            "(160) train_accuracy : 13.28125\n",
            "  Batch   160  of    209.    Elapsed: 0:03:37.\n",
            "(170) train_accuracy : 13.051470588235293\n",
            "  Batch   170  of    209.    Elapsed: 0:03:51.\n",
            "(180) train_accuracy : 13.020833333333334\n",
            "  Batch   180  of    209.    Elapsed: 0:04:04.\n",
            "(190) train_accuracy : 12.993421052631579\n",
            "  Batch   190  of    209.    Elapsed: 0:04:18.\n",
            "(200) train_accuracy : 13.125\n",
            "  Batch   200  of    209.    Elapsed: 0:04:32.\n",
            "Epoch 78 Complete\n",
            "    epoch        acc\n",
            "0       1  91.849148\n",
            "1       2  91.301703\n",
            "2       3  93.309002\n",
            "3       4  84.610706\n",
            "4       5  92.944039\n",
            "5       6  92.944039\n",
            "6       7  70.681265\n",
            "7       8  59.367397\n",
            "8       9  51.277372\n",
            "9      10  44.221411\n",
            "10     11  44.647202\n",
            "11     12  42.518248\n",
            "12     13  14.841849\n",
            "13     14  14.841849\n",
            "14     15  14.841849\n",
            "15     16  14.841849\n",
            "16     17  14.841849\n",
            "17     18  14.841849\n",
            "18     19  14.841849\n",
            "19     20  14.841849\n",
            "20     21  14.841849\n",
            "21     22  15.571776\n",
            "22     23  13.807786\n",
            "23     24  14.841849\n",
            "24     25  15.571776\n",
            "25     26  14.841849\n",
            "26     27  13.807786\n",
            "27     28  11.982968\n",
            "28     29  13.807786\n",
            "29     30  13.807786\n",
            "30     31  15.571776\n",
            "31     32  15.571776\n",
            "32     33  15.571776\n",
            "33     34  14.841849\n",
            "34     35  14.841849\n",
            "35     36  14.841849\n",
            "36     37  14.841849\n",
            "37     38  14.841849\n",
            "38     39  13.807786\n",
            "39     40  15.571776\n",
            "40     41  13.199513\n",
            "41     42  12.712895\n",
            "42     43  13.199513\n",
            "43     44  15.571776\n",
            "44     45  14.841849\n",
            "45     46  15.571776\n",
            "46     47  13.807786\n",
            "47     48  15.571776\n",
            "48     49  13.199513\n",
            "49     50  15.571776\n",
            "50     51  14.841849\n",
            "51     52  14.841849\n",
            "52     53  14.841849\n",
            "53     54  15.571776\n",
            "54     55  14.841849\n",
            "55     56  14.841849\n",
            "56     57  14.841849\n",
            "57     58  14.841849\n",
            "58     59  15.571776\n",
            "59     60  14.841849\n",
            "60     61  13.807786\n",
            "61     62  15.571776\n",
            "62     63  15.571776\n",
            "63     64  15.571776\n",
            "64     65  15.571776\n",
            "65     66  14.841849\n",
            "66     67  15.571776\n",
            "67     68  13.807786\n",
            "68     69  15.571776\n",
            "69     70  14.841849\n",
            "70     71  12.712895\n",
            "71     72  15.571776\n",
            "72     73  14.841849\n",
            "73     74  15.571776\n",
            "74     75  13.807786\n",
            "75     76  15.571776\n",
            "76     77  15.571776\n",
            "77     78  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 79 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 15.625\n",
            "  Batch    10  of    209.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 17.1875\n",
            "  Batch    20  of    209.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 16.666666666666668\n",
            "  Batch    30  of    209.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 17.1875\n",
            "  Batch    40  of    209.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 18.125\n",
            "  Batch    50  of    209.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 18.229166666666668\n",
            "  Batch    60  of    209.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 18.303571428571427\n",
            "  Batch    70  of    209.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 17.96875\n",
            "  Batch    80  of    209.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 17.708333333333332\n",
            "  Batch    90  of    209.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 17.5\n",
            "  Batch   100  of    209.    Elapsed: 0:02:16.\n",
            "(110) train_accuracy : 17.045454545454547\n",
            "  Batch   110  of    209.    Elapsed: 0:02:30.\n",
            "(120) train_accuracy : 17.96875\n",
            "  Batch   120  of    209.    Elapsed: 0:02:43.\n",
            "(130) train_accuracy : 17.307692307692307\n",
            "  Batch   130  of    209.    Elapsed: 0:02:57.\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.3.2\n",
        "!rm -r ./semi_supervised_learning_with_gan ;git clone https://github.com/iamardian/semi_supervised_learning_with_gan.git\n",
        "# !python ./semi_supervised_learning_with_gan/ssgan_parsbert.py\n",
        "!python ./semi_supervised_learning_with_gan/ecgan_parsbert.py -d persiannews -p 0.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MxznXs1WoSc-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}