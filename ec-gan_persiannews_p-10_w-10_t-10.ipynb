{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semi_gan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamardian/digikalamag/blob/main/ec-gan_persiannews_p-10_w-10_t-10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAH9liVXvM8M",
        "outputId": "1708f5b3-d349-4386-e2d3-878d6977caec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVzghhQznhMw",
        "outputId": "da493ac1-3630-478f-ffbc-a5c5d39af131"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/075_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/076_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "\n",
            "======== Epoch 78 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 78 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "evaluation\n",
            "evaluation : 151 / 1331 * 100 = 11.344853493613824 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "validation\n",
            "validate : 178 / 1480 * 100 = 12.027027027027028 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/076_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/077_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "\n",
            "======== Epoch 79 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 79 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/077_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/078_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "\n",
            "======== Epoch 80 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 80 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/078_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/079_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "\n",
            "======== Epoch 81 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 81 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/079_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/080_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "\n",
            "======== Epoch 82 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 82 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "evaluation\n",
            "evaluation : 335 / 1331 * 100 = 25.169045830202858 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "validation\n",
            "validate : 329 / 1480 * 100 = 22.22972972972973 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/080_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/081_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "\n",
            "======== Epoch 83 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 83 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "evaluation\n",
            "evaluation : 209 / 1331 * 100 = 15.702479338842975 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "validation\n",
            "validate : 219 / 1480 * 100 = 14.797297297297296 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/081_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/082_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "\n",
            "======== Epoch 84 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 84 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "evaluation\n",
            "evaluation : 209 / 1331 * 100 = 15.702479338842975 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "validation\n",
            "validate : 219 / 1480 * 100 = 14.797297297297296 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/082_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/083_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "\n",
            "======== Epoch 85 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 85 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "evaluation\n",
            "evaluation : 209 / 1331 * 100 = 15.702479338842975 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "validation\n",
            "validate : 219 / 1480 * 100 = 14.797297297297296 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/083_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/084_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "\n",
            "======== Epoch 86 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:40.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 86 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "evaluation\n",
            "evaluation : 209 / 1331 * 100 = 15.702479338842975 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "validation\n",
            "validate : 219 / 1480 * 100 = 14.797297297297296 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/084_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/085_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "\n",
            "======== Epoch 87 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 87 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/085_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/086_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "\n",
            "======== Epoch 88 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 88 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/086_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/087_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "\n",
            "======== Epoch 89 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 89 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/087_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/088_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "\n",
            "======== Epoch 90 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 90 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "89     90  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "89     90  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/088_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/089_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "\n",
            "======== Epoch 91 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:40.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 91 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "89     90  16.153268\n",
            "90     91  13.523666\n",
            "evaluation\n",
            "evaluation : 180 / 1331 * 100 = 13.5236664162284 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "89     90  15.540541\n",
            "90     91  13.243243\n",
            "validation\n",
            "validate : 196 / 1480 * 100 = 13.243243243243244 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/089_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/090_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "\n",
            "======== Epoch 92 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 92 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "89     90  16.153268\n",
            "90     91  13.523666\n",
            "91     92  13.974455\n",
            "evaluation\n",
            "evaluation : 186 / 1331 * 100 = 13.974455296769348 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "89     90  15.540541\n",
            "90     91  13.243243\n",
            "91     92  13.783784\n",
            "validation\n",
            "validate : 204 / 1480 * 100 = 13.783783783783784 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/090_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/091_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "\n",
            "======== Epoch 93 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:40.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 93 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "89     90  16.153268\n",
            "90     91  13.523666\n",
            "91     92  13.974455\n",
            "92     93  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "89     90  15.540541\n",
            "90     91  13.243243\n",
            "91     92  13.783784\n",
            "92     93  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/091_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/092_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "\n",
            "======== Epoch 94 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 94 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "89     90  16.153268\n",
            "90     91  13.523666\n",
            "91     92  13.974455\n",
            "92     93  16.153268\n",
            "93     94  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "89     90  15.540541\n",
            "90     91  13.243243\n",
            "91     92  13.783784\n",
            "92     93  15.540541\n",
            "93     94  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/092_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/093_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "\n",
            "======== Epoch 95 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 95 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "89     90  16.153268\n",
            "90     91  13.523666\n",
            "91     92  13.974455\n",
            "92     93  16.153268\n",
            "93     94  16.153268\n",
            "94     95  15.702479\n",
            "evaluation\n",
            "evaluation : 209 / 1331 * 100 = 15.702479338842975 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "89     90  15.540541\n",
            "90     91  13.243243\n",
            "91     92  13.783784\n",
            "92     93  15.540541\n",
            "93     94  15.540541\n",
            "94     95  14.797297\n",
            "validation\n",
            "validate : 219 / 1480 * 100 = 14.797297297297296 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/093_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/094_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "\n",
            "======== Epoch 96 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 96 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "89     90  16.153268\n",
            "90     91  13.523666\n",
            "91     92  13.974455\n",
            "92     93  16.153268\n",
            "93     94  16.153268\n",
            "94     95  15.702479\n",
            "95     96  13.974455\n",
            "evaluation\n",
            "evaluation : 186 / 1331 * 100 = 13.974455296769348 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "89     90  15.540541\n",
            "90     91  13.243243\n",
            "91     92  13.783784\n",
            "92     93  15.540541\n",
            "93     94  15.540541\n",
            "94     95  14.797297\n",
            "95     96  13.783784\n",
            "validation\n",
            "validate : 204 / 1480 * 100 = 13.783783783783784 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/094_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/095_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "\n",
            "======== Epoch 97 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 97 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "89     90  16.153268\n",
            "90     91  13.523666\n",
            "91     92  13.974455\n",
            "92     93  16.153268\n",
            "93     94  16.153268\n",
            "94     95  15.702479\n",
            "95     96  13.974455\n",
            "96     97  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "89     90  15.540541\n",
            "90     91  13.243243\n",
            "91     92  13.783784\n",
            "92     93  15.540541\n",
            "93     94  15.540541\n",
            "94     95  14.797297\n",
            "95     96  13.783784\n",
            "96     97  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/095_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/096_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "\n",
            "======== Epoch 98 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 98 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "89     90  16.153268\n",
            "90     91  13.523666\n",
            "91     92  13.974455\n",
            "92     93  16.153268\n",
            "93     94  16.153268\n",
            "94     95  15.702479\n",
            "95     96  13.974455\n",
            "96     97  16.153268\n",
            "97     98  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "89     90  15.540541\n",
            "90     91  13.243243\n",
            "91     92  13.783784\n",
            "92     93  15.540541\n",
            "93     94  15.540541\n",
            "94     95  14.797297\n",
            "95     96  13.783784\n",
            "96     97  15.540541\n",
            "97     98  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/096_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/097_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "\n",
            "======== Epoch 99 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 99 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "89     90  16.153268\n",
            "90     91  13.523666\n",
            "91     92  13.974455\n",
            "92     93  16.153268\n",
            "93     94  16.153268\n",
            "94     95  15.702479\n",
            "95     96  13.974455\n",
            "96     97  16.153268\n",
            "97     98  16.153268\n",
            "98     99  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "89     90  15.540541\n",
            "90     91  13.243243\n",
            "91     92  13.783784\n",
            "92     93  15.540541\n",
            "93     94  15.540541\n",
            "94     95  14.797297\n",
            "95     96  13.783784\n",
            "96     97  15.540541\n",
            "97     98  15.540541\n",
            "98     99  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/097_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/098_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "\n",
            "======== Epoch 100 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 100 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "89     90  16.153268\n",
            "90     91  13.523666\n",
            "91     92  13.974455\n",
            "92     93  16.153268\n",
            "93     94  16.153268\n",
            "94     95  15.702479\n",
            "95     96  13.974455\n",
            "96     97  16.153268\n",
            "97     98  16.153268\n",
            "98     99  16.153268\n",
            "99    100  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "89     90  15.540541\n",
            "90     91  13.243243\n",
            "91     92  13.783784\n",
            "92     93  15.540541\n",
            "93     94  15.540541\n",
            "94     95  14.797297\n",
            "95     96  13.783784\n",
            "96     97  15.540541\n",
            "97     98  15.540541\n",
            "98     99  15.540541\n",
            "99    100  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/098_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/099_persiannews_0.1_0.1_0.1.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.1/best_model.pth']\n",
            "call load_best_model\n",
            "call test\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_test_accuracy\n",
            "test\n",
            "         acc\n",
            "0  94.221411\n",
            "test\n",
            "test : 1549 / 1644 * 100 = 94.2214111922141 \n",
            "       train data evaluation validation data evaluation\n",
            "0     [1, 64.31254695717506]    [1, 60.608108108108105]\n",
            "1     [2, 87.15251690458302]     [2, 83.24324324324324]\n",
            "2      [3, 95.0413223140496]     [3, 90.13513513513513]\n",
            "3     [4, 92.56198347107438]     [4, 88.51351351351352]\n",
            "4     [5, 97.29526671675433]     [5, 92.63513513513514]\n",
            "..                       ...                        ...\n",
            "95  [96, 13.974455296769348]   [96, 13.783783783783784]\n",
            "96   [97, 16.15326821938392]    [97, 15.54054054054054]\n",
            "97   [98, 16.15326821938392]    [98, 15.54054054054054]\n",
            "98   [99, 16.15326821938392]    [99, 15.54054054054054]\n",
            "99  [100, 16.15326821938392]   [100, 15.54054054054054]\n",
            "\n",
            "[100 rows x 2 columns]\n",
            "   test data evaluation\n",
            "0             94.221411\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.3.2\n",
        "!rm -r ./semi_supervised_learning_with_gan ;git clone https://github.com/iamardian/semi_supervised_learning_with_gan.git\n",
        "# !python ./semi_supervised_learning_with_gan/ssgan_parsbert.py -d digikalamag -p 0.01\n",
        "!python ./semi_supervised_learning_with_gan/ecgan_parsbert.py -d persiannews -p 0.1 -w 0.1 -t 0.1 -e 100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MxznXs1WoSc-"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}